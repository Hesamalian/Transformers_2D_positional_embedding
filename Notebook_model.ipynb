{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1337d63-2728-4d38-8ab1-9ff56396b344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has very small number of 100 samples\n",
      "The model has 21,204 trainable parameters\n",
      "Epoch 1/50, Train_loss: 0.270, Train_accuracy: 26.000\n",
      "Epoch 2/50, Train_loss: 0.266, Train_accuracy: 26.000\n",
      "Epoch 3/50, Train_loss: 0.264, Train_accuracy: 30.000\n",
      "Epoch 4/50, Train_loss: 0.263, Train_accuracy: 38.000\n",
      "Epoch 5/50, Train_loss: 0.261, Train_accuracy: 45.000\n",
      "Epoch 6/50, Train_loss: 0.260, Train_accuracy: 51.000\n",
      "Epoch 7/50, Train_loss: 0.260, Train_accuracy: 54.000\n",
      "Epoch 8/50, Train_loss: 0.259, Train_accuracy: 61.000\n",
      "Epoch 9/50, Train_loss: 0.258, Train_accuracy: 61.000\n",
      "Epoch 10/50, Train_loss: 0.257, Train_accuracy: 66.000\n",
      "Epoch 11/50, Train_loss: 0.256, Train_accuracy: 67.000\n",
      "Epoch 12/50, Train_loss: 0.255, Train_accuracy: 68.000\n",
      "Epoch 13/50, Train_loss: 0.255, Train_accuracy: 68.000\n",
      "Epoch 14/50, Train_loss: 0.254, Train_accuracy: 68.000\n",
      "Epoch 15/50, Train_loss: 0.252, Train_accuracy: 67.000\n",
      "Epoch 16/50, Train_loss: 0.251, Train_accuracy: 66.000\n",
      "Epoch 17/50, Train_loss: 0.249, Train_accuracy: 66.000\n",
      "Epoch 18/50, Train_loss: 0.248, Train_accuracy: 63.000\n",
      "Epoch 19/50, Train_loss: 0.245, Train_accuracy: 62.000\n",
      "Epoch 20/50, Train_loss: 0.242, Train_accuracy: 61.000\n",
      "Epoch 21/50, Train_loss: 0.238, Train_accuracy: 59.000\n",
      "Epoch 22/50, Train_loss: 0.234, Train_accuracy: 61.000\n",
      "Epoch 23/50, Train_loss: 0.230, Train_accuracy: 63.000\n",
      "Epoch 24/50, Train_loss: 0.225, Train_accuracy: 65.000\n",
      "Epoch 25/50, Train_loss: 0.221, Train_accuracy: 67.000\n",
      "Epoch 26/50, Train_loss: 0.216, Train_accuracy: 70.000\n",
      "Epoch 27/50, Train_loss: 0.211, Train_accuracy: 77.000\n",
      "Epoch 28/50, Train_loss: 0.206, Train_accuracy: 81.000\n",
      "Epoch 29/50, Train_loss: 0.202, Train_accuracy: 84.000\n",
      "Epoch 30/50, Train_loss: 0.199, Train_accuracy: 83.000\n",
      "Epoch 31/50, Train_loss: 0.196, Train_accuracy: 85.000\n",
      "Epoch 32/50, Train_loss: 0.193, Train_accuracy: 87.000\n",
      "Epoch 33/50, Train_loss: 0.191, Train_accuracy: 87.000\n",
      "Epoch 34/50, Train_loss: 0.189, Train_accuracy: 88.000\n",
      "Epoch 35/50, Train_loss: 0.186, Train_accuracy: 88.000\n",
      "Epoch 36/50, Train_loss: 0.185, Train_accuracy: 90.000\n",
      "Epoch 37/50, Train_loss: 0.183, Train_accuracy: 90.000\n",
      "Epoch 38/50, Train_loss: 0.182, Train_accuracy: 91.000\n",
      "Epoch 39/50, Train_loss: 0.180, Train_accuracy: 92.000\n",
      "Epoch 40/50, Train_loss: 0.179, Train_accuracy: 92.000\n",
      "Epoch 41/50, Train_loss: 0.177, Train_accuracy: 93.000\n",
      "Epoch 42/50, Train_loss: 0.175, Train_accuracy: 93.000\n",
      "Epoch 43/50, Train_loss: 0.174, Train_accuracy: 93.000\n",
      "Epoch 44/50, Train_loss: 0.173, Train_accuracy: 93.000\n",
      "Epoch 45/50, Train_loss: 0.171, Train_accuracy: 94.000\n",
      "Epoch 46/50, Train_loss: 0.170, Train_accuracy: 94.000\n",
      "Epoch 47/50, Train_loss: 0.168, Train_accuracy: 95.000\n",
      "Epoch 48/50, Train_loss: 0.167, Train_accuracy: 95.000\n",
      "Epoch 49/50, Train_loss: 0.165, Train_accuracy: 95.000\n",
      "Epoch 50/50, Train_loss: 0.164, Train_accuracy: 95.000\n"
     ]
    }
   ],
   "source": [
    "#Transformer with self attention and 2D-positional embeddings for reading form-like documents\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "class SelfAttentionWide(nn.Module):\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        self.tokeys = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.tovalues = nn.Linear(emb, emb * heads, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
    "\n",
    "        keys = self.tokeys(x).view(b, t, h, e)\n",
    "        queries = self.toqueries(x).view(b, t, h, e)\n",
    "        values = self.tovalues(x).view(b, t, h, e)\n",
    "\n",
    "        # compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "\n",
    "        queries = queries / (e ** (1 / 4))\n",
    "        keys = keys / (e ** (1 / 4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b * h, t, t)\n",
    "\n",
    "        if self.mask:  # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n",
    "\n",
    "        return self.unifyheads(out)[0]\n",
    "\n",
    "\n",
    "def make_context_vector(sentence, vocab):\n",
    "    sent_temp = []\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            sent_temp.append(vocab[word])\n",
    "        else:\n",
    "            sent_temp.append(vocab['<RARE/>'])       \n",
    "    return torch.tensor(sent_temp, dtype=torch.long)\n",
    "\n",
    "\n",
    "class EMB(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, NUM_Field):\n",
    "        super(EMB, self).__init__()\n",
    "\n",
    "        # positional embedding\n",
    "        self.activation_pos_x = nn.ReLU()\n",
    "        self.activation_pos_y = nn.ReLU()\n",
    "        self.dropout_x = nn.Dropout(p=0.1)\n",
    "        self.dropout_y = nn.Dropout(p=0.1)\n",
    "        self.linear_pos_x = nn.Linear(1, int(embedding_dim / 2))\n",
    "        self.linear_pos_y = nn.Linear(1, int(embedding_dim / 2))\n",
    "\n",
    "        # self attention\n",
    "        self.self_attention = SelfAttentionWide(2 * embedding_dim, 1)\n",
    "        self.activation_att_expand = nn.ReLU()\n",
    "        self.activation_att_shrink = nn.ReLU()\n",
    "        self.linear_att_expand = nn.Linear(2 * embedding_dim, 4 * 2 * embedding_dim)\n",
    "        self.linear_att_shrink = nn.Linear(4 * 2 * embedding_dim, 2 * embedding_dim)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False)\n",
    "        self.norm2 = nn.LayerNorm(2 * embedding_dim, elementwise_affine=False)\n",
    "\n",
    "        # candidate postion\n",
    "        self.linear_box = nn.Linear(2, embedding_dim)\n",
    "        self.activation_box = nn.ReLU()\n",
    "        self.linear_last = nn.Linear(3 * embedding_dim, embedding_dim)\n",
    "\n",
    "        # tokens embedding\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(embedding_dim, 256)\n",
    "        self.linear2 = nn.Linear(256, embedding_dim)\n",
    "        # self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "        self.cos = nn.CosineSimilarity(dim=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.emb = nn.Embedding(NUM_Field, embedding_dim)\n",
    "        self.linear_emb1 = nn.Linear(embedding_dim, 256)\n",
    "        self.linear_emb2 = nn.Linear(256, embedding_dim)\n",
    "        self.linear3 = nn.Linear(embedding_dim, NUM_Field)\n",
    "\n",
    "    def forward(self, inputs_tokens, field_tensor, cand_pos, inputs_pos):\n",
    "        inputs_pos = -inputs_pos\n",
    "        inputs_pos_x = self.activation_pos_x(inputs_pos[:, 0].unsqueeze(1))\n",
    "        inputs_pos_x = self.linear_pos_x(inputs_pos_x)\n",
    "        inputs_pos_y = self.activation_pos_y(inputs_pos[:, 1].unsqueeze(1))\n",
    "        inputs_pos_y = self.linear_pos_y(inputs_pos_y)\n",
    "        inputs_pos = torch.cat((inputs_pos_x, inputs_pos_y), 1)\n",
    "\n",
    "        cand_position = self.linear_box(cand_pos)\n",
    "        cand_position = self.norm(cand_position)\n",
    "        candidate = self.embeddings(inputs_tokens)\n",
    "        candidate = torch.cat((candidate, inputs_pos), 1)\n",
    "        attn_output_next = self.self_attention(candidate.unsqueeze(0))\n",
    "        embed_candidate = torch.max(attn_output_next, 0).values.view(1, -1)\n",
    "        embed_candidate = self.norm2(embed_candidate)\n",
    "\n",
    "        cand_full = torch.cat((embed_candidate, cand_position), 1)\n",
    "        cand_full = self.activation_box(cand_full)\n",
    "        cand_full = self.linear_last(cand_full)\n",
    "        embed_field = self.emb(field_tensor)\n",
    "        out = self.cos(cand_full, embed_field)\n",
    "        out = self.sig(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "vocab=json.load(open('vocab.json'))\n",
    "vocab_size = len(vocab)\n",
    "num_labels = 2\n",
    "num_field = 2\n",
    "labels = {\"Positive\": 1, \"Negative\": 0}\n",
    "field_id = {\"date\": 0, \"total\": 1}\n",
    "num_dim=10\n",
    "num_epochs=50\n",
    "\n",
    "\n",
    "train_data=pd.read_csv('small_sample_training.csv','\\t')\n",
    "train_data['cand_pos'] = train_data['cand_pos'].apply(literal_eval)\n",
    "train_data['tokens_pos'] = train_data['tokens_pos'].apply(literal_eval)\n",
    "train_data['can_tokens'] = train_data['can_tokens'].apply(literal_eval)\n",
    "\n",
    "print(f'Training has very small number of {len(train_data):,} samples')\n",
    "weight_classes=torch.FloatTensor([sum(train_data['can_targets']=='Positive')/len(train_data), sum(train_data['can_targets']=='Negative')/len(train_data)])    \n",
    "\n",
    "    \n",
    "model = EMB(vocab_size, num_dim,num_field)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#TRAINING\n",
    "model.train()\n",
    "logs = {}\n",
    "for epoch in range(num_epochs):\n",
    "    train_total_loss = 0\n",
    "    train_correct=0\n",
    "    test_total_loss_total = 0\n",
    "    test_correct_total=0\n",
    "    test_pred_total=[]\n",
    "    test_target_total=[]\n",
    "    test_total_loss_date = 0\n",
    "    test_correct_date=0\n",
    "    test_pred_date=[]\n",
    "    test_target_date=[]\n",
    "    model.train() \n",
    "    for i,row in train_data.iterrows():\n",
    "        optimizer.zero_grad()\n",
    "        context_vector = make_context_vector(row['can_tokens'], vocab)\n",
    "        inputs_pos=torch.tensor(row['tokens_pos'], dtype=torch.float32)\n",
    "        field_tensor = torch.tensor([field_id[row['can_labels']]], dtype=torch.long)\n",
    "        cand_tensor=torch.tensor(row['cand_pos'], dtype=torch.float32).view(1, -1)\n",
    "        target=torch.tensor([labels[row['can_targets']]]).to(torch.float32)\n",
    "        log_probs = model(context_vector,field_tensor,cand_tensor,inputs_pos)\n",
    "        loss=loss_function(log_probs, target)*(weight_classes[labels[row['can_targets']]])\n",
    "        train_total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        train_correct += (log_probs.round() == target).float().sum()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / len(train_data)\n",
    "    train_loss=train_total_loss/len(train_data)\n",
    "    print(\"Epoch {}/{}, Train_loss: {:.3f}, Train_accuracy: {:.3f}\".format(epoch+1, num_epochs,train_loss, train_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
